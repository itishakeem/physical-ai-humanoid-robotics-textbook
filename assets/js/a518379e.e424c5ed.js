"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[9002],{6489:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapter5/diagrams","title":"Chapter 5: AI Integration in Humanoids - Diagrams / Illustrations","description":"Perception-Cognition-Action Pipeline","source":"@site/docs/chapter5/diagrams.md","sourceDirName":"chapter5","slug":"/chapter5/diagrams","permalink":"/docs/chapter5/diagrams","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter5/diagrams.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: AI Integration in Humanoids - Practical Examples","permalink":"/docs/chapter5/practical_examples"},"next":{"title":"Chapter 6: Motion and Navigation","permalink":"/docs/chapter6/summary"}}');var o=r(4848),i=r(8453);const a={},s="Chapter 5: AI Integration in Humanoids - Diagrams / Illustrations",c={},l=[{value:"Perception-Cognition-Action Pipeline",id:"perception-cognition-action-pipeline",level:2},{value:"Deep Learning Architecture for Humanoid Control",id:"deep-learning-architecture-for-humanoid-control",level:2},{value:"Reinforcement Learning Training Loop",id:"reinforcement-learning-training-loop",level:2},{value:"Vision-Language Integration",id:"vision-language-integration",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",p:"p",pre:"pre",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-5-ai-integration-in-humanoids---diagrams--illustrations",children:"Chapter 5: AI Integration in Humanoids - Diagrams / Illustrations"})}),"\n",(0,o.jsx)(e.h2,{id:"perception-cognition-action-pipeline",children:"Perception-Cognition-Action Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"This comprehensive diagram illustrates the complete flow of information in an AI-integrated humanoid robot."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    EXTERNAL ENVIRONMENT                     \u2502\r\n\u2502                                                             \u2502\r\n\u2502    Objects, Humans, Obstacles, Tasks, Unexpected Events    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                            \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                   PERCEPTION STAGE (AI: Vision/Fusion)      \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Sensors:                                                   \u2502\r\n\u2502   \u2022 RGB Camera: Raw pixels (640\xd7480\xd73 = 921,600 values)   \u2502\r\n\u2502   \u2022 Depth Camera: 3D point cloud (640\xd7480\xd73 = 900K points)\u2502\r\n\u2502   \u2022 LiDAR: Environmental map (100K+ laser points)         \u2502\r\n\u2502   \u2022 Proprioception: Joint angles, IMU (40+ sensor values) \u2502\r\n\u2502                                                             \u2502\r\n\u2502  AI Processing:                                             \u2502\r\n\u2502   CNN: Images \u2192 Object detection                           \u2502\r\n\u2502   PointNet: Point clouds \u2192 3D scene understanding          \u2502\r\n\u2502   Sensor Fusion: Combine multi-modal data (100 ms)         \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Output: Semantic scene representation                      \u2502\r\n\u2502   [Humans: [list], Objects: [list], Obstacles: [list]]     \u2502\r\n\u2502   [Self-Pose: {position, orientation}]                    \u2502\r\n\u2502   [Object-Properties: {color, shape, material, ...}]       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                            \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502              COGNITION STAGE (AI: Reasoning/Planning)       \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Symbolic Reasoning:                                        \u2502\r\n\u2502   \u2022 Task decomposition: "Pick cup" \u2192 [Locate, Reach,      \u2502\r\n\u2502     Grasp, Lift, Carry, Place]                             \u2502\r\n\u2502   \u2022 Knowledge base queries: "Where is water source?"        \u2502\r\n\u2502   \u2022 Logical inference: "If cup is hot, use tongs"           \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Learned Prediction Models:                                 \u2502\r\n\u2502   \u2022 Vision-Language: "What can I do with this object?"     \u2502\r\n\u2502   \u2022 Dynamics: "What happens if I pull that?"               \u2502\r\n\u2502   \u2022 Outcome prediction: "Will this grasp succeed?"          \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Planning Algorithms:                                       \u2502\r\n\u2502   \u2022 Path planning (RRT): Arm trajectory in 3D space        \u2502\r\n\u2502   \u2022 Motion planning (collision avoidance)                   \u2502\r\n\u2502   \u2022 Temporal planning (task sequencing)                     \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Decision Making (RL Policy):                               \u2502\r\n\u2502   \u2022 Neural network \u03c0(action | state)                        \u2502\r\n\u2502   \u2022 Outputs: Grasp pose, arm trajectory, motor commands     \u2502\r\n\u2502   \u2022 Considers: Success probability, energy cost, safety     \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Output: Executable action plan                             \u2502\r\n\u2502   [Action-sequence, Parameters, Success-probability]        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                            \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502              ACTION STAGE (Control: Execution)              \u2502\r\n\u2502                                                             \u2502\r\n\u2502  High-Level Coordination:                                   \u2502\r\n\u2502   \u2022 Whole-body control QP solver                            \u2502\r\n\u2502   \u2022 Prioritize: Balance > primary task > efficiency         \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Mid-Level Trajectory Execution:                            \u2502\r\n\u2502   \u2022 Inverse kinematics: Target pose \u2192 Joint angles         \u2502\r\n\u2502   \u2022 Trajectory generator: Smooth path over time             \u2502\r\n\u2502   \u2022 Adapt to perturbations (wind, collisions)              \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Low-Level Motor Control:                                   \u2502\r\n\u2502   \u2022 PID controller per joint (1000 Hz)                      \u2502\r\n\u2502   \u2022 Real-time sensor feedback (encoders, force/torque)     \u2502\r\n\u2502   \u2022 Torque commands to actuators                            \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Output: Actuator commands                                  \u2502\r\n\u2502   [Motor1-PWM, Motor2-PWM, ..., Motor40-PWM]               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                            \u2193\r\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                \u2502    PHYSICAL EXECUTION    \u2502\r\n                \u2502                          \u2502\r\n                \u2502  \u2022 Joints rotate         \u2502\r\n                \u2502  \u2022 Limbs move            \u2502\r\n                \u2502  \u2022 Forces applied        \u2502\r\n                \u2502  \u2022 Environment interaction\u2502\r\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                            \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502              FEEDBACK & LEARNING STAGE                      \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Immediate Feedback (Real-time Control Loop):               \u2502\r\n\u2502   \u2022 Success/failure of current action                       \u2502\r\n\u2502   \u2022 Sensor readback: Did joint move as expected?            \u2502\r\n\u2502   \u2022 Force feedback: Contact information                     \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Outcome Evaluation:                                        \u2502\r\n\u2502   \u2022 Did task succeed or fail?                              \u2502\r\n\u2502   \u2022 Quantify performance: Time, energy, accuracy            \u2502\r\n\u2502   \u2022 Record: State, action taken, result                    \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Learning (Offline, typically at night):                    \u2502\r\n\u2502   \u2022 Store experience: (s, a, r, s\') tuples                 \u2502\r\n\u2502   \u2022 Update vision model: Improve object detection           \u2502\r\n\u2502   \u2022 Update grasp model: Improve grasp prediction            \u2502\r\n\u2502   \u2022 Update policy: Improve action selection                 \u2502\r\n\u2502   \u2022 Update dynamics model: Refine world prediction          \u2502\r\n\u2502                                                             \u2502\r\n\u2502  Result: Better performance on next attempt                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                            \u2191\r\n                   [LOOP BACK TO PERCEPTION]\r\n        Cycle time: 10-100 ms (depending on task level)\r\n\r\nLegend:\r\n\u2500\u2192 Information flow (1000s of bits)\r\nData volume: Perception ~1GB/hour, Cognition ~1MB/hour, Action ~1MB/hour\r\nComputation: GPU for perception/learning, CPU for control\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"deep-learning-architecture-for-humanoid-control",children:"Deep Learning Architecture for Humanoid Control"}),"\n",(0,o.jsx)(e.p,{children:"This diagram shows how neural networks transform sensor data into motor commands."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"SENSOR INPUT LAYER\r\n    \u2193\r\n    Vision Input: Image (640\xd7480\xd73)\r\n    \u251c\u2500 RGB Data: 921,600 pixel values (0-255)\r\n    \u2514\u2500 0.1-0.5 MB per frame\r\n\r\n    Proprioceptive Input: Joint state (40\xd73 = 120 values)\r\n    \u251c\u2500 Joint angles: 40 values\r\n    \u251c\u2500 Joint velocities: 40 values\r\n    \u251c\u2500 Motor currents: 40 values\r\n    \u2514\u2500 ~1 KB total\r\n\r\n    \u2193\u2193\u2193\r\n\r\nFEATURE EXTRACTION (CNN)\r\n    \u251c\u2500 Conv Layer 1: [640\xd7480\xd73] \u2192 [320\xd7240\xd764]\r\n    \u251c\u2500 Conv Layer 2: [320\xd7240\xd764] \u2192 [160\xd7120\xd7128]\r\n    \u251c\u2500 Conv Layer 3: [160\xd7120\xd7128] \u2192 [80\xd760\xd7256]\r\n    \u251c\u2500 Conv Layer 4: [80\xd760\xd7256] \u2192 [40\xd730\xd7512]\r\n    \u2502\r\n    \u2514\u2500 Output: 512 feature channels\r\n       (Learned representation, ~10KB)\r\n\r\n    \u2193\u2193\u2193\r\n\r\nPROPRIOCEPTIVE PROCESSING (Shallow MLP)\r\n    \u251c\u2500 Layer 1: 120 inputs \u2192 256 hidden units\r\n    \u251c\u2500 Layer 2: 256 \u2192 128 hidden units\r\n    \u2502\r\n    \u2514\u2500 Output: 128-dim proprioceptive features\r\n\r\n    \u2193\u2193\u2193\r\n\r\nFUSION LAYER\r\n    \u251c\u2500 Concatenate: Vision features (512) + Proprioception (128)\r\n    \u251c\u2500 Combined: 640-dim fused feature vector\r\n    \u2502\r\n    \u2514\u2500 Process: Apply attention to determine which features matter most\r\n\r\n    \u2193\u2193\u2193\r\n\r\nPOLICY NETWORK (Main Decision-Maker)\r\n    \u251c\u2500 Layer 1: 640 inputs \u2192 512 hidden (ReLU activation)\r\n    \u251c\u2500 Layer 2: 512 \u2192 256 hidden (ReLU)\r\n    \u251c\u2500 Layer 3: 256 \u2192 128 hidden (ReLU)\r\n    \u2502\r\n    \u2514\u2500 Output Layer: 128 \u2192 80 action outputs\r\n       40 targets for joint angles (degrees)\r\n       40 gains for each joint (stiffness control)\r\n\r\nMOTOR COMMAND GENERATION\r\n    \u251c\u2500 For each joint i:\r\n    \u2502  target_angle[i] = output[i]\r\n    \u2502  stiffness[i] = output[40+i]\r\n    \u2502\r\n    \u2514\u2500 Apply torque control:\r\n       \u03c4[i] = stiffness[i] \xd7 (target - current_angle)\r\n\r\n    \u2193\u2193\u2193\r\n\r\nMOTOR OUTPUT\r\n    \u251c\u2500 Joint 1: PWM = 128 (50% duty cycle)\r\n    \u251c\u2500 Joint 2: PWM = 200 (78% duty cycle)\r\n    \u251c\u2500 ...\r\n    \u2514\u2500 Joint 40: PWM = 64 (25% duty cycle)\r\n\r\nNETWORK STATISTICS:\r\n- Total parameters: ~15 million\r\n- Memory: ~60 MB\r\n- Inference time: 50-100 ms on GPU, 200-500 ms on CPU\r\n- Training data: 1-10 million examples\r\n- Training time: 1-4 weeks on single GPU\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"reinforcement-learning-training-loop",children:"Reinforcement Learning Training Loop"}),"\n",(0,o.jsx)(e.p,{children:"This diagram shows how robots learn through trial and error."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"REINFORCEMENT LEARNING CYCLE (Typical robot training)\r\n\r\nSTEP 1: ENVIRONMENT STATE\r\n    State s(t):\r\n    \u251c\u2500 Robot joint positions\r\n    \u251c\u2500 Object positions\r\n    \u251c\u2500 Target location\r\n    \u2514\u2500 Agent goal\r\n\r\nSTEP 2: POLICY DECISION (Neural network)\r\n    Policy \u03c0(a | s):\r\n    \u251c\u2500 Input: Current state s(t)\r\n    \u251c\u2500 Neural network processes\r\n    \u2514\u2500 Output: Probability distribution over actions\r\n\r\nSTEP 3: ACTION SELECTION\r\n    Sample action a(t) from policy:\r\n    \u251c\u2500 80% chance: Grasp-reach action\r\n    \u251c\u2500 15% chance: Explore action (random)\r\n    \u2514\u2500 5% chance: Fallback action\r\n\r\nSTEP 4: ENVIRONMENT RESPONSE\r\n    Execute action, observe:\r\n    \u251c\u2500 Next state s(t+1)\r\n    \u251c\u2500 Reward r(t)\r\n    \u2514\u2500 Done flag (episode completed?)\r\n\r\n    Reward examples:\r\n    \u251c\u2500 Successful grasp: r = +100\r\n    \u251c\u2500 Each timestep: r = -1 (encourage speed)\r\n    \u251c\u2500 Collision: r = -50\r\n    \u2514\u2500 Dropped object: r = -100\r\n\r\nSTEP 5: REPLAY BUFFER\r\n    Store experience tuple: (s, a, r, s', done)\r\n    \u251c\u2500 Replay buffer size: 1,000,000 experiences\r\n    \u251c\u2500 Samples for training drawn from this buffer\r\n    \u2514\u2500 Allows off-policy learning (learn from past data)\r\n\r\nSTEP 6: LEARNING (Periodic, offline)\r\n    Sample mini-batch of 64 experiences from replay buffer\r\n    \r\n    For each experience (s, a, r, s', done):\r\n        Compute target value:\r\n        Q_target = r + \u03b3 * max_a' Q(s', a')  [if not done]\r\n        Q_target = r                         [if done]\r\n        \r\n        Compute prediction:\r\n        Q_pred = Q(s, a)\r\n        \r\n        Loss:\r\n        L = (Q_pred - Q_target)\xb2\r\n        \r\n    Update network weights:\r\n    \u03b8 \u2190 \u03b8 - \u03b1 \u2207L\r\n    \r\n    Training: 10-100 mini-batch updates per episode\r\n\r\nSTEP 7: PERFORMANCE TRACKING\r\n    Plot metrics over training:\r\n    \r\n    Episode Reward (sample trajectory)\r\n    \u251c\u2500 Episode 0-100: Average reward = -80 (mostly failures)\r\n    \u251c\u2500 Episode 1000: Average reward = +20 (learning!)\r\n    \u251c\u2500 Episode 10000: Average reward = +70 (good skill)\r\n    \u2514\u2500 Episode 100000: Average reward = +90 (expert level)\r\n    \r\n    Success Rate:\r\n    \u251c\u2500 Episode 0: 10% success (random)\r\n    \u251c\u2500 Episode 1000: 30% success\r\n    \u251c\u2500 Episode 10000: 80% success\r\n    \u2514\u2500 Episode 100000: 95% success (converged)\r\n\r\nREPEAT for millions of episodes...\r\n\r\nFINAL RESULT:\r\n    Trained policy \u03c0(a | s) that achieves:\r\n    \u251c\u2500 95% success rate on training task\r\n    \u251c\u2500 Generalizes to 60-80% success on similar tasks\r\n    \u2514\u2500 Can be deployed on real robot\r\n\r\nKEY INSIGHTS:\r\n- Early training: Mostly random exploration\r\n- Middle training: Gradual improvement as patterns emerge\r\n- Late training: Fine-tuning and convergence\r\n- Typical training: 1-4 weeks on GPU cluster\r\n- Sim-to-real transfer: Often requires domain randomization\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,o.jsx)(e.p,{children:"This diagram shows how robots understand scenes described in natural language."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'INPUT:\r\n  Image:      [Visual perception of scene]\r\n  Language:   "Hand me the blue cup near the window"\r\n\r\nVISION PROCESSING (CNN encoder):\r\n  Image \u2192 Feature extraction \u2192 Visual features\r\n  [Objects detected: Cup_red, Cup_blue, Window, Shelf, etc.]\r\n  [Spatial relationships detected: Near, On, Above, etc.]\r\n\r\nLANGUAGE PROCESSING (Text encoder):\r\n  Text \u2192 Tokenization \u2192 Token embeddings \u2192 Contextual encoding\r\n  Tokens: ["Hand", "me", "the", "blue", "cup", "near", "window"]\r\n  \r\n  Language understanding:\r\n  \u251c\u2500 Action: HAND_OVER (give object to person)\r\n  \u251c\u2500 Object: blue_cup\r\n  \u251c\u2500 Spatial constraint: near_window\r\n  \u2514\u2500 Recipient: human\r\n\r\nVISION-LANGUAGE ALIGNMENT (Learned model):\r\n  Project both modalities to shared space:\r\n  \r\n  Visual features: [cup_red_embedding, cup_blue_embedding, window_embedding]\r\n  Language query: [blue_cup_near_window_embedding]\r\n  \r\n  Matching: Find visual features closest to language query\r\n  \u251c\u2500 cup_blue matches "blue" (very high similarity)\r\n  \u251c\u2500 window matches "near window" (spatial reasoning)\r\n  \u2514\u2500 Final match: cup_blue near window (highest combined score)\r\n\r\nDECISION (Policy network):\r\n  Inputs: [Identified object location, Recipient location]\r\n  Outputs: [Arm trajectory, Grip configuration, Delivery location]\r\n  \r\n  Decision tree:\r\n  \u251c\u2500 Object is fragile? \u2192 Use gentle grip (low force)\r\n  \u251c\u2500 Object is far? \u2192 Lean forward, extend arm fully\r\n  \u251c\u2500 Path to human blocked? \u2192 Replan around obstacle\r\n  \u2514\u2500 Human ready to receive? \u2192 Extend object smoothly\r\n\r\nACTION EXECUTION:\r\n  Arm moves to cup\r\n  Gripper closes with measured force\r\n  Arm moves to human\r\n  Wait for human to take\r\n  Gripper opens gently\r\n  \r\nRESULT: Cup successfully delivered to human\n'})})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var t=r(6540);const o={},i=t.createContext(o);function a(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);